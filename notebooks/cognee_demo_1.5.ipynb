{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T14:02:47.336283Z",
     "start_time": "2024-09-20T14:02:43.652444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from typing import Union\n",
    "\n",
    "from cognee.modules.cognify.config import get_cognify_config\n",
    "from cognee.shared.data_models import KnowledgeGraph\n",
    "from cognee.modules.data.models import Dataset, Data\n",
    "from cognee.modules.data.methods.get_dataset_data import get_dataset_data\n",
    "from cognee.modules.data.methods import get_datasets, get_datasets_by_name\n",
    "from cognee.modules.pipelines.tasks.Task import Task\n",
    "from cognee.modules.pipelines import run_tasks, run_tasks_parallel\n",
    "from cognee.modules.users.models import User\n",
    "from cognee.modules.users.methods import get_default_user\n",
    "from cognee.modules.pipelines.operations.get_pipeline_status import get_pipeline_status\n",
    "from cognee.modules.pipelines.operations.log_pipeline_status import log_pipeline_status\n",
    "from cognee.tasks import chunk_extract_summary, \\\n",
    "    chunk_naive_llm_classifier, \\\n",
    "    chunk_remove_disconnected, \\\n",
    "    infer_data_ontology, \\\n",
    "    save_chunks_to_store, \\\n",
    "    chunk_update_check, \\\n",
    "    chunks_into_graph, \\\n",
    "    source_documents_to_chunks, \\\n",
    "    check_permissions_on_documents, \\\n",
    "    classify_documents\n",
    "\n",
    "logger = logging.getLogger(\"cognify.v2\")\n",
    "\n",
    "update_status_lock = asyncio.Lock()"
   ],
   "id": "958375a6ffc0c2e4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T14:02:48.519686Z",
     "start_time": "2024-09-20T14:02:48.515589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "job_position = \"\"\"Senior Data Scientist (Machine Learning)\n",
    "\n",
    "Company: TechNova Solutions\n",
    "Location: San Francisco, CA\n",
    "\n",
    "Job Description:\n",
    "\n",
    "TechNova Solutions is seeking a Senior Data Scientist specializing in Machine Learning to join our dynamic analytics team. The ideal candidate will have a strong background in developing and deploying machine learning models, working with large datasets, and translating complex data into actionable insights.\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "Develop and implement advanced machine learning algorithms and models.\n",
    "Analyze large, complex datasets to extract meaningful patterns and insights.\n",
    "Collaborate with cross-functional teams to integrate predictive models into products.\n",
    "Stay updated with the latest advancements in machine learning and data science.\n",
    "Mentor junior data scientists and provide technical guidance.\n",
    "Qualifications:\n",
    "\n",
    "Master’s or Ph.D. in Data Science, Computer Science, Statistics, or a related field.\n",
    "5+ years of experience in data science and machine learning.\n",
    "Proficient in Python, R, and SQL.\n",
    "Experience with deep learning frameworks (e.g., TensorFlow, PyTorch).\n",
    "Strong problem-solving skills and attention to detail.\n",
    "Candidate CVs\n",
    "\"\"\"\n"
   ],
   "id": "df16431d0f48b006",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T14:02:49.120838Z",
     "start_time": "2024-09-20T14:02:49.118294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "job_1 = \"\"\"\n",
    "CV 1: Relevant\n",
    "Name: Dr. Emily Carter\n",
    "Contact Information:\n",
    "\n",
    "Email: emily.carter@example.com\n",
    "Phone: (555) 123-4567\n",
    "Summary:\n",
    "\n",
    "Senior Data Scientist with over 8 years of experience in machine learning and predictive analytics. Expertise in developing advanced algorithms and deploying scalable models in production environments.\n",
    "\n",
    "Education:\n",
    "\n",
    "Ph.D. in Computer Science, Stanford University (2014)\n",
    "B.S. in Mathematics, University of California, Berkeley (2010)\n",
    "Experience:\n",
    "\n",
    "Senior Data Scientist, InnovateAI Labs (2016 – Present)\n",
    "Led a team in developing machine learning models for natural language processing applications.\n",
    "Implemented deep learning algorithms that improved prediction accuracy by 25%.\n",
    "Collaborated with cross-functional teams to integrate models into cloud-based platforms.\n",
    "Data Scientist, DataWave Analytics (2014 – 2016)\n",
    "Developed predictive models for customer segmentation and churn analysis.\n",
    "Analyzed large datasets using Hadoop and Spark frameworks.\n",
    "Skills:\n",
    "\n",
    "Programming Languages: Python, R, SQL\n",
    "Machine Learning: TensorFlow, Keras, Scikit-Learn\n",
    "Big Data Technologies: Hadoop, Spark\n",
    "Data Visualization: Tableau, Matplotlib\n",
    "\"\"\""
   ],
   "id": "9086abf3af077ab4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T14:02:49.675003Z",
     "start_time": "2024-09-20T14:02:49.671615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "job_2 = \"\"\"\n",
    "CV 2: Relevant\n",
    "Name: Michael Rodriguez\n",
    "Contact Information:\n",
    "\n",
    "Email: michael.rodriguez@example.com\n",
    "Phone: (555) 234-5678\n",
    "Summary:\n",
    "\n",
    "Data Scientist with a strong background in machine learning and statistical modeling. Skilled in handling large datasets and translating data into actionable business insights.\n",
    "\n",
    "Education:\n",
    "\n",
    "M.S. in Data Science, Carnegie Mellon University (2013)\n",
    "B.S. in Computer Science, University of Michigan (2011)\n",
    "Experience:\n",
    "\n",
    "Senior Data Scientist, Alpha Analytics (2017 – Present)\n",
    "Developed machine learning models to optimize marketing strategies.\n",
    "Reduced customer acquisition cost by 15% through predictive modeling.\n",
    "Data Scientist, TechInsights (2013 – 2017)\n",
    "Analyzed user behavior data to improve product features.\n",
    "Implemented A/B testing frameworks to evaluate product changes.\n",
    "Skills:\n",
    "\n",
    "Programming Languages: Python, Java, SQL\n",
    "Machine Learning: Scikit-Learn, XGBoost\n",
    "Data Visualization: Seaborn, Plotly\n",
    "Databases: MySQL, MongoDB\n",
    "\"\"\""
   ],
   "id": "a9de0cc07f798b7f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T14:02:50.286828Z",
     "start_time": "2024-09-20T14:02:50.284369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "job_3 = \"\"\"\n",
    "CV 3: Relevant\n",
    "Name: Sarah Nguyen\n",
    "Contact Information:\n",
    "\n",
    "Email: sarah.nguyen@example.com\n",
    "Phone: (555) 345-6789\n",
    "Summary:\n",
    "\n",
    "Data Scientist specializing in machine learning with 6 years of experience. Passionate about leveraging data to drive business solutions and improve product performance.\n",
    "\n",
    "Education:\n",
    "\n",
    "M.S. in Statistics, University of Washington (2014)\n",
    "B.S. in Applied Mathematics, University of Texas at Austin (2012)\n",
    "Experience:\n",
    "\n",
    "Data Scientist, QuantumTech (2016 – Present)\n",
    "Designed and implemented machine learning algorithms for financial forecasting.\n",
    "Improved model efficiency by 20% through algorithm optimization.\n",
    "Junior Data Scientist, DataCore Solutions (2014 – 2016)\n",
    "Assisted in developing predictive models for supply chain optimization.\n",
    "Conducted data cleaning and preprocessing on large datasets.\n",
    "Skills:\n",
    "\n",
    "Programming Languages: Python, R\n",
    "Machine Learning Frameworks: PyTorch, Scikit-Learn\n",
    "Statistical Analysis: SAS, SPSS\n",
    "Cloud Platforms: AWS, Azure\n",
    "\"\"\""
   ],
   "id": "185ff1c102d06111",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T14:02:50.950343Z",
     "start_time": "2024-09-20T14:02:50.946378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "job_4 = \"\"\"\n",
    "CV 4: Not Relevant\n",
    "Name: David Thompson\n",
    "Contact Information:\n",
    "\n",
    "Email: david.thompson@example.com\n",
    "Phone: (555) 456-7890\n",
    "Summary:\n",
    "\n",
    "Creative Graphic Designer with over 8 years of experience in visual design and branding. Proficient in Adobe Creative Suite and passionate about creating compelling visuals.\n",
    "\n",
    "Education:\n",
    "\n",
    "B.F.A. in Graphic Design, Rhode Island School of Design (2012)\n",
    "Experience:\n",
    "\n",
    "Senior Graphic Designer, CreativeWorks Agency (2015 – Present)\n",
    "Led design projects for clients in various industries.\n",
    "Created branding materials that increased client engagement by 30%.\n",
    "Graphic Designer, Visual Innovations (2012 – 2015)\n",
    "Designed marketing collateral, including brochures, logos, and websites.\n",
    "Collaborated with the marketing team to develop cohesive brand strategies.\n",
    "Skills:\n",
    "\n",
    "Design Software: Adobe Photoshop, Illustrator, InDesign\n",
    "Web Design: HTML, CSS\n",
    "Specialties: Branding and Identity, Typography\n",
    "\"\"\""
   ],
   "id": "d55ce4c58f8efb67",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T14:02:51.548191Z",
     "start_time": "2024-09-20T14:02:51.545520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "job_5 = \"\"\"\n",
    "CV 5: Not Relevant\n",
    "Name: Jessica Miller\n",
    "Contact Information:\n",
    "\n",
    "Email: jessica.miller@example.com\n",
    "Phone: (555) 567-8901\n",
    "Summary:\n",
    "\n",
    "Experienced Sales Manager with a strong track record in driving sales growth and building high-performing teams. Excellent communication and leadership skills.\n",
    "\n",
    "Education:\n",
    "\n",
    "B.A. in Business Administration, University of Southern California (2010)\n",
    "Experience:\n",
    "\n",
    "Sales Manager, Global Enterprises (2015 – Present)\n",
    "Managed a sales team of 15 members, achieving a 20% increase in annual revenue.\n",
    "Developed sales strategies that expanded customer base by 25%.\n",
    "Sales Representative, Market Leaders Inc. (2010 – 2015)\n",
    "Consistently exceeded sales targets and received the 'Top Salesperson' award in 2013.\n",
    "Skills:\n",
    "\n",
    "Sales Strategy and Planning\n",
    "Team Leadership and Development\n",
    "CRM Software: Salesforce, Zoho\n",
    "Negotiation and Relationship Building\n",
    "\"\"\""
   ],
   "id": "ca4ecc32721ad332",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T14:02:54.243987Z",
     "start_time": "2024-09-20T14:02:52.498195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cognee\n",
    "from os import listdir, path\n",
    "\n",
    "data_path = path.abspath(\".data\")\n",
    "\n",
    "results = await cognee.add([job_1, job_2,job_3,job_4,job_5,job_position], \"example\")\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ],
   "id": "904df61ba484a8e5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sys:1: SAWarning: TypeDecorator UUID() will not produce a cache key because the ``cache_ok`` attribute is not set to True.  This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions.  Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this warning at: https://sqlalche.me/e/20/cprf)\n",
      "2024-09-20 16:02:52,880|[WARNING]|53377|8480345088|dlt|utils.py|resolve_merge_strategy:223|Destination does not support any merge strategies and `merge` write disposition  for table `file_metadata` cannot be met and will fall back to `append`. Change write disposition.2024-09-20 16:02:53,216|[WARNING]|53377|8480345088|dlt|utils.py|resolve_merge_strategy:223|Destination does not support any merge strategies and `merge` write disposition  for table `file_metadata` cannot be met and will fall back to `append`. Change write disposition."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dlt.pipeline.pipeline.Pipeline object at 0x166676710>\n",
      "{'1726840972.7469912': [{'started_at': DateTime(2024, 9, 20, 14, 2, 53, 214773, tzinfo=Timezone('UTC')), 'finished_at': DateTime(2024, 9, 20, 14, 2, 54, 239978, tzinfo=Timezone('UTC')), 'job_metrics': {'file_metadata.a5a9170f4c.typed-jsonl': LoadJobMetrics(job_id='file_metadata.a5a9170f4c.typed-jsonl', file_path='/Users/vasa/.dlt/pipelines/file_load_from_filesystem/load/normalized/1726840972.7469912/started_jobs/file_metadata.a5a9170f4c.0.typed-jsonl', table_name='file_metadata', started_at=DateTime(2024, 9, 20, 14, 2, 53, 220755, tzinfo=Timezone('UTC')), finished_at=DateTime(2024, 9, 20, 14, 2, 53, 222174, tzinfo=Timezone('UTC')), state='completed', remote_url=None)}}]}\n",
      "dlt.destinations.sqlalchemy\n",
      "sqlite:///cognee_db\n",
      "sqlalchemy\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "example\n",
      "['1726840972.7469912']\n",
      "[LoadPackageInfo(load_id='1726840972.7469912', package_path='/Users/vasa/.dlt/pipelines/file_load_from_filesystem/load/loaded/1726840972.7469912', state='loaded', schema=Schema file_load_from_filesystem at 6038968784, schema_update={}, completed_at=DateTime(2024, 9, 20, 14, 2, 54, 235948, tzinfo=Timezone('UTC')), jobs={'failed_jobs': [], 'started_jobs': [], 'completed_jobs': [LoadJobInfo(state='completed_jobs', file_path='/Users/vasa/.dlt/pipelines/file_load_from_filesystem/load/loaded/1726840972.7469912/completed_jobs/file_metadata.a5a9170f4c.0.typed-jsonl', file_size=670, created_at=DateTime(2024, 9, 20, 14, 2, 52, 881922, tzinfo=Timezone('UTC')), elapsed=1.3540260791778564, job_file_info=ParsedLoadJobFileName(table_name='file_metadata', file_id='a5a9170f4c', retry_count=0, file_format='typed-jsonl'), failed_message=None)], 'new_jobs': []})]\n",
      "False\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T14:02:55.564445Z",
     "start_time": "2024-09-20T14:02:55.562784Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6f9b564de121713d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T14:02:56.714408Z",
     "start_time": "2024-09-20T14:02:56.711812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from enum import Enum, auto\n",
    "# from typing import Optional, List, Union, Dict, Any\n",
    "# from pydantic import BaseModel, Field\n",
    "# \n",
    "# class Node(BaseModel):\n",
    "#     \"\"\"Node in a knowledge graph.\"\"\"\n",
    "#     id: str\n",
    "#     name: str\n",
    "#     type: str\n",
    "#     description: str\n",
    "#     properties: Optional[Dict[str, Any]] = Field(None, description = \"A dictionary of properties associated with the node.\")\n",
    "# \n",
    "# class Edge(BaseModel):\n",
    "#     \"\"\"Edge in a knowledge graph.\"\"\"\n",
    "#     source_node_id: str\n",
    "#     target_node_id: str\n",
    "#     relationship_name: str\n",
    "#     properties: Optional[Dict[str, Any]] = Field(None, description = \"A dictionary of properties associated with the edge.\")\n",
    "# \n",
    "# class KnowledgeGraph(BaseModel):\n",
    "#     \"\"\"Knowledge graph.\"\"\"\n",
    "#     nodes: List[Node] = Field(..., default_factory=list)\n",
    "#     edges: List[Edge] = Field(..., default_factory=list)"
   ],
   "id": "8911f8bd4f8c440a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T14:02:57.925667Z",
     "start_time": "2024-09-20T14:02:57.922353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "async def run_cognify_pipeline(dataset: Dataset, user: User = None):\n",
    "    data_documents: list[Data] = await get_dataset_data(dataset_id = dataset.id)\n",
    "\n",
    "    try:\n",
    "\n",
    "        root_node_id = None\n",
    "\n",
    "        tasks = [\n",
    "            \n",
    "            Task(check_permissions_on_documents, user = user, permissions = [\"write\"]),\n",
    "            Task(infer_data_ontology, root_node_id = root_node_id, ontology_model = KnowledgeGraph),\n",
    "            Task(source_documents_to_chunks, parent_node_id = root_node_id), # Classify documents and save them as a nodes in graph db, extract text chunks based on the document type\n",
    "            Task(chunks_into_graph, graph_model = KnowledgeGraph, collection_name = \"entities\", task_config = { \"batch_size\": 10 }), # Generate knowledge graphs from the document chunks and attach it to chunk nodes\n",
    "            Task(chunk_update_check, collection_name = \"chunks\"), # Find all affected chunks, so we don't process unchanged chunks\n",
    "            Task(\n",
    "                save_chunks_to_store,\n",
    "                collection_name = \"chunks\",\n",
    "            ), \n",
    "            Task(chunk_remove_disconnected), # Remove the obsolete document chunks.\n",
    "        ]\n",
    "\n",
    "        pipeline = run_tasks(tasks, data_documents)\n",
    "\n",
    "        async for result in pipeline:\n",
    "            print(result)\n",
    "    except Exception as error:\n",
    "        raise error"
   ],
   "id": "7c431fdef4921ae0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T14:02:58.905774Z",
     "start_time": "2024-09-20T14:02:58.625915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user = await get_default_user()\n",
    "datasets = await get_datasets_by_name([\"example\"], user.id)\n",
    "await run_cognify_pipeline(datasets[0], user)"
   ],
   "id": "f0a91b99c6215e09",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error occurred while running async generator task: `infer_data_ontology`\n",
      "'Data' object has no attribute 'file_path'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vasa/Projects/cognee/.venv/lib/python3.11/site-packages/cognee/modules/pipelines/operations/run_tasks.py\", line 26, in run_tasks\n",
      "    async for partial_result in async_iterator:\n",
      "  File \"/Users/vasa/Projects/cognee/.venv/lib/python3.11/site-packages/cognee/tasks/infer_data_ontology/infer_data_ontology.py\", line 176, in infer_data_ontology\n",
      "    root_node_id = await ontology_engine.add_graph_ontology(documents = documents)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasa/Projects/cognee/.venv/lib/python3.11/site-packages/cognee/tasks/infer_data_ontology/infer_data_ontology.py\", line 92, in add_graph_ontology\n",
      "    with open(base_file.file_path, \"rb\") as file:\n",
      "              ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Data' object has no attribute 'file_path'Error occurred while running coroutine task: `check_permissions_on_documents`\n",
      "'Data' object has no attribute 'file_path'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vasa/Projects/cognee/.venv/lib/python3.11/site-packages/cognee/modules/pipelines/operations/run_tasks.py\", line 86, in run_tasks\n",
      "    async for result in run_tasks(leftover_tasks, task_result):\n",
      "  File \"/Users/vasa/Projects/cognee/.venv/lib/python3.11/site-packages/cognee/modules/pipelines/operations/run_tasks.py\", line 49, in run_tasks\n",
      "    raise error\n",
      "  File \"/Users/vasa/Projects/cognee/.venv/lib/python3.11/site-packages/cognee/modules/pipelines/operations/run_tasks.py\", line 26, in run_tasks\n",
      "    async for partial_result in async_iterator:\n",
      "  File \"/Users/vasa/Projects/cognee/.venv/lib/python3.11/site-packages/cognee/tasks/infer_data_ontology/infer_data_ontology.py\", line 176, in infer_data_ontology\n",
      "    root_node_id = await ontology_engine.add_graph_ontology(documents = documents)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasa/Projects/cognee/.venv/lib/python3.11/site-packages/cognee/tasks/infer_data_ontology/infer_data_ontology.py\", line 92, in add_graph_ontology\n",
      "    with open(base_file.file_path, \"rb\") as file:\n",
      "              ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Data' object has no attribute 'file_path'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Data' object has no attribute 'file_path'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m user \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m get_default_user()\n\u001B[1;32m      2\u001B[0m datasets \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m get_datasets_by_name([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexample\u001B[39m\u001B[38;5;124m\"\u001B[39m], user\u001B[38;5;241m.\u001B[39mid)\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m run_cognify_pipeline(datasets[\u001B[38;5;241m0\u001B[39m], user)\n",
      "Cell \u001B[0;32mIn[10], line 27\u001B[0m, in \u001B[0;36mrun_cognify_pipeline\u001B[0;34m(dataset, user)\u001B[0m\n\u001B[1;32m     25\u001B[0m         \u001B[38;5;28mprint\u001B[39m(result)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m---> 27\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
      "Cell \u001B[0;32mIn[10], line 24\u001B[0m, in \u001B[0;36mrun_cognify_pipeline\u001B[0;34m(dataset, user)\u001B[0m\n\u001B[1;32m      8\u001B[0m     tasks \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      9\u001B[0m         \n\u001B[1;32m     10\u001B[0m         Task(check_permissions_on_documents, user \u001B[38;5;241m=\u001B[39m user, permissions \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m]),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     19\u001B[0m         Task(chunk_remove_disconnected), \u001B[38;5;66;03m# Remove the obsolete document chunks.\u001B[39;00m\n\u001B[1;32m     20\u001B[0m     ]\n\u001B[1;32m     22\u001B[0m     pipeline \u001B[38;5;241m=\u001B[39m run_tasks(tasks, data_documents)\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m pipeline:\n\u001B[1;32m     25\u001B[0m         \u001B[38;5;28mprint\u001B[39m(result)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
      "File \u001B[0;32m~/Projects/cognee/.venv/lib/python3.11/site-packages/cognee/modules/pipelines/operations/run_tasks.py:97\u001B[0m, in \u001B[0;36mrun_tasks\u001B[0;34m(tasks, data)\u001B[0m\n\u001B[1;32m     90\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[1;32m     91\u001B[0m         logger\u001B[38;5;241m.\u001B[39merror(\n\u001B[1;32m     92\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError occurred while running coroutine task: `\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     93\u001B[0m             running_task\u001B[38;5;241m.\u001B[39mexecutable\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n\u001B[1;32m     94\u001B[0m             \u001B[38;5;28mstr\u001B[39m(error),\n\u001B[1;32m     95\u001B[0m             exc_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     96\u001B[0m         )\n\u001B[0;32m---> 97\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39misfunction(running_task\u001B[38;5;241m.\u001B[39mexecutable):\n\u001B[1;32m    100\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRunning function task: `\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m, running_task\u001B[38;5;241m.\u001B[39mexecutable\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
      "File \u001B[0;32m~/Projects/cognee/.venv/lib/python3.11/site-packages/cognee/modules/pipelines/operations/run_tasks.py:86\u001B[0m, in \u001B[0;36mrun_tasks\u001B[0;34m(tasks, data)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     84\u001B[0m     task_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m running_task\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;241m*\u001B[39margs)\n\u001B[0;32m---> 86\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m run_tasks(leftover_tasks, task_result):\n\u001B[1;32m     87\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m result\n\u001B[1;32m     89\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFinished coroutine task: `\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m, running_task\u001B[38;5;241m.\u001B[39mexecutable\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
      "File \u001B[0;32m~/Projects/cognee/.venv/lib/python3.11/site-packages/cognee/modules/pipelines/operations/run_tasks.py:49\u001B[0m, in \u001B[0;36mrun_tasks\u001B[0;34m(tasks, data)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[1;32m     43\u001B[0m         logger\u001B[38;5;241m.\u001B[39merror(\n\u001B[1;32m     44\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError occurred while running async generator task: `\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     45\u001B[0m             running_task\u001B[38;5;241m.\u001B[39mexecutable\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n\u001B[1;32m     46\u001B[0m             \u001B[38;5;28mstr\u001B[39m(error),\n\u001B[1;32m     47\u001B[0m             exc_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     48\u001B[0m         )\n\u001B[0;32m---> 49\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39misgeneratorfunction(running_task\u001B[38;5;241m.\u001B[39mexecutable):\n\u001B[1;32m     52\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRunning generator task: `\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m, running_task\u001B[38;5;241m.\u001B[39mexecutable\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
      "File \u001B[0;32m~/Projects/cognee/.venv/lib/python3.11/site-packages/cognee/modules/pipelines/operations/run_tasks.py:26\u001B[0m, in \u001B[0;36mrun_tasks\u001B[0;34m(tasks, data)\u001B[0m\n\u001B[1;32m     22\u001B[0m results \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     24\u001B[0m async_iterator \u001B[38;5;241m=\u001B[39m running_task\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;241m*\u001B[39margs)\n\u001B[0;32m---> 26\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m partial_result \u001B[38;5;129;01min\u001B[39;00m async_iterator:\n\u001B[1;32m     27\u001B[0m     results\u001B[38;5;241m.\u001B[39mappend(partial_result)\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(results) \u001B[38;5;241m==\u001B[39m next_task_batch_size:\n",
      "File \u001B[0;32m~/Projects/cognee/.venv/lib/python3.11/site-packages/cognee/tasks/infer_data_ontology/infer_data_ontology.py:176\u001B[0m, in \u001B[0;36minfer_data_ontology\u001B[0;34m(documents, ontology_model, root_node_id)\u001B[0m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ontology_model \u001B[38;5;241m==\u001B[39m KnowledgeGraph:\n\u001B[1;32m    175\u001B[0m     ontology_engine \u001B[38;5;241m=\u001B[39m OntologyEngine()\n\u001B[0;32m--> 176\u001B[0m     root_node_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m ontology_engine\u001B[38;5;241m.\u001B[39madd_graph_ontology(documents \u001B[38;5;241m=\u001B[39m documents)\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    178\u001B[0m     graph_engine \u001B[38;5;241m=\u001B[39m get_graph_engine()\n",
      "File \u001B[0;32m~/Projects/cognee/.venv/lib/python3.11/site-packages/cognee/tasks/infer_data_ontology/infer_data_ontology.py:92\u001B[0m, in \u001B[0;36mOntologyEngine.add_graph_ontology\u001B[0;34m(self, file_path, documents)\u001B[0m\n\u001B[1;32m     89\u001B[0m chunk_strategy \u001B[38;5;241m=\u001B[39m chunk_config\u001B[38;5;241m.\u001B[39mchunk_strategy\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m base_file \u001B[38;5;129;01min\u001B[39;00m documents:\n\u001B[0;32m---> 92\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[43mbase_file\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfile_path\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[1;32m     93\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     94\u001B[0m             file_type \u001B[38;5;241m=\u001B[39m guess_file_type(file)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Data' object has no attribute 'file_path'"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "async def cognify(datasets: Union[str, list[str]] = None, user: User = None):\n",
    "    if user is None:\n",
    "        user = await get_default_user()\n",
    "\n",
    "    existing_datasets = await get_datasets(user.id)\n",
    "\n",
    "    if datasets is None or len(datasets) == 0:\n",
    "        # If no datasets are provided, cognify all existing datasets.\n",
    "        datasets = existing_datasets\n",
    "\n",
    "    if type(datasets[0]) == str:\n",
    "        datasets = await get_datasets_by_name(datasets, user.id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    existing_datasets_map = {\n",
    "        dataset.name: True for dataset in existing_datasets\n",
    "    }\n",
    "\n",
    "    awaitables = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        # print(\"here is the dataset\", dataset)\n",
    "        dataset_name = dataset.name\n",
    "\n",
    "        if dataset_name in existing_datasets_map:\n",
    "            awaitables.append(run_cognify_pipeline(dataset, user))\n",
    "\n",
    "    return await asyncio.gather(*awaitables)\n",
    "\n"
   ],
   "id": "e7d4f03f7dab9807",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "await cognify(\"example\")",
   "id": "d9248a01352964e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6afc6307bd115dbe",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
