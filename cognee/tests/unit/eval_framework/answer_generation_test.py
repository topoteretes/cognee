import pytest
from evals.eval_framework.answer_generation.answer_generation_executor import (
    AnswerGeneratorExecutor,
    question_answering_engine_options,
)
from evals.eval_framework.benchmark_adapters.dummy_adapter import DummyAdapter
import cognee


@pytest.mark.parametrize("qa_engine", question_answering_engine_options.values())
@pytest.mark.asyncio
async def test_answer_generation(qa_engine):
    limit = 1
    corpus_list, qa_pairs = DummyAdapter().load_corpus(limit=limit)

    await cognee.prune.prune_data()
    await cognee.prune.prune_system(metadata=True)
    await cognee.add(corpus_list)
    await cognee.cognify()

    answer_generator = AnswerGeneratorExecutor()
    answers = await answer_generator.question_answering_non_parallel(
        questions=qa_pairs,
        answer_resolver=qa_engine,
    )
    assert len(answers) == len(qa_pairs)
    assert answers[0]["question"] == qa_pairs[0]["question"], (
        "AnswerGeneratorExecutor is passing the question incorrectly"
    )
    assert answers[0]["golden_answer"] == qa_pairs[0]["answer"], (
        "AnswerGeneratorExecutor is passing the golden answer incorrectly"
    )
    assert len(answers[0]["answer"]) > 0, (
        "The answer generated by AnswerGeneratorExecutor is empty."
    )
