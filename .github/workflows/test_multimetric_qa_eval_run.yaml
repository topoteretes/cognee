name: test | multimetric qa eval run

on:
  workflow_dispatch:
  pull_request:
    types: [labeled, synchronize]


concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  run_multimetric_qa_eval_test:
      uses: ./.github/workflows/reusable_python_example.yml
      with:
        example-location: ./evals/multimetric_qa_eval_run.py
        arguments: "--params_file evals/qa_eval_parameters.json --out_dir dirname"
      secrets:
        LLM_MODEL: ${{ secrets.LLM_MODEL }}
        LLM_ENDPOINT: ${{ secrets.LLM_ENDPOINT }}
        LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
        LLM_API_VERSION: ${{ secrets.LLM_API_VERSION }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }} # Until we add support for azure for DeepEval
        EMBEDDING_MODEL: ${{ secrets.EMBEDDING_MODEL }}
        EMBEDDING_ENDPOINT: ${{ secrets.EMBEDDING_ENDPOINT }}
        EMBEDDING_API_KEY: ${{ secrets.EMBEDDING_API_KEY }}
        EMBEDDING_API_VERSION: ${{ secrets.EMBEDDING_API_VERSION }}
        GRAPHISTRY_USERNAME: ${{ secrets.GRAPHISTRY_USERNAME }}
        GRAPHISTRY_PASSWORD: ${{ secrets.GRAPHISTRY_PASSWORD }}
