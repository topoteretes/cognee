name: test | ollama

on:
  workflow_call:

jobs:

  run_llama-cpp_test:

    # needs 32 Gb RAM for phi4 in a container
    runs-on: buildjet-8vcpu-ubuntu-2204

    steps:
      - name: Checkout repository
        uses: actions/checkout@v6

      - name: Cognee Setup
        uses: ./.github/actions/cognee_setup
        with:
          python-version: '3.11.x'

      - name: Install torch dependency
        run: |
          uv add torch

      - name: Install nix
        run: |
          sh <(curl --proto '=https' --tlsv1.2 -L https://nixos.org/nix/install) --no-daemon

      - name: Install llama-cpp
        run: nix profile install nixpkgs#llama-cpp

      - name: Pull required Ollama models
        run: |
          curl -X POST http://localhost:11434/api/pull -d '{"name": "phi4"}'
          curl -X POST http://localhost:11434/api/pull -d '{"name": "avr/sfr-embedding-mistral:latest"}'

      - name: Run example test
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          PYTHONFAULTHANDLER: 1
          LLM_PROVIDER: "llama-cpp"
          LLM_API_KEY: "ollama"
          LLM_ENDPOINT: "http://localhost:11434/v1/"
          LLM_MODEL: "phi4"
          EMBEDDING_PROVIDER: "llama-cpp"
          EMBEDDING_MODEL: "avr/sfr-embedding-mistral:latest"
          EMBEDDING_ENDPOINT: "http://localhost:11434/api/embed"
          EMBEDDING_DIMENSIONS: "4096"
          HUGGINGFACE_TOKENIZER: "Salesforce/SFR-Embedding-Mistral"
        run: uv run python ./examples/python/simple_example.py
